{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyZZXkxsG3A7",
        "outputId": "0414ce57-9d31-418a-919e-bc126608b451"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifer=pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JDmf_xXsHE07"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer ,TFAutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G1mRDxpHVdy",
        "outputId": "d46b5d90-74f4-4afe-e26a-cc39f5127c68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "model=TFAutoModelForSequenceClassification.from_pretrained(model_name,from_pt=True)\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "classifier=pipeline('sentiment-analysis',model=model_name,tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k57_gRELMoVR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path=\"NLP_Transformers_Using_huggingFace\\Datasets\\spamham.csv\"\n",
        "df=pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GYvIIAV4NTxM"
      },
      "outputs": [],
      "source": [
        "X=list(df[\"sms\"])\n",
        "y=list(df[\"label\"])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6EcUUP9-Lfhz"
      },
      "outputs": [],
      "source": [
        "#how to fine tune for preTrained Models\n",
        "# 1) call pre trained models\n",
        "#2) call for tokenizer for respect to pretrained models\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0N1av2fbOxEI"
      },
      "outputs": [],
      "source": [
        "train_encodings=tokenizer(X_train,truncation=True,padding=True)\n",
        "test_encodings=tokenizer(X_test,truncation=True,padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vVo-7WCUPDlS"
      },
      "outputs": [],
      "source": [
        "#3) convert encoding into data set objects\n",
        "import tensorflow as tf\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ_eI23JPxpy",
        "outputId": "e9888aa9-9ed2-448c-a302-76b0e7f5373b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(238,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(238,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lV7g_nCGP8i5"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification , TFTrainingArguments\n",
        "training_args=TFTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp6zeQxRT63i"
      },
      "outputs": [],
      "source": [
        "from transformers import TFTrainer\n",
        "with training_args.strategy.scope():\n",
        "  model=TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "  trainer=TFTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=test_datset\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2HR2B9ZULxx"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
